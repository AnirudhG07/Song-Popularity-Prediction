{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de723c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error, \n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, roc_curve, confusion_matrix, classification_report)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load train and test data\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Convert to binary classification\n",
    "y_train_binary = (train_df['song_popularity'] > 0.37).astype(int)\n",
    "print(f\"Target distribution: {np.bincount(y_train_binary)}\")\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"Find optimal threshold using Youden's J statistic\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "def advanced_feature_engineering(X):\n",
    "    \"\"\"Advanced feature engineering with domain knowledge\"\"\"\n",
    "    X_featured = X.copy()\n",
    "    \n",
    "    # Basic audio features\n",
    "    numerical_features = ['song_duration_ms', 'acousticness', 'danceability', 'energy', \n",
    "                         'instrumentalness', 'liveness', 'loudness', 'speechiness', \n",
    "                         'tempo', 'audio_valence']\n",
    "    \n",
    "    # 1. Interaction features (domain knowledge)\n",
    "    X_featured['energy_danceability'] = X_featured['energy'] * X_featured['danceability']\n",
    "    X_featured['valence_energy'] = X_featured['audio_valence'] * X_featured['energy']\n",
    "    X_featured['acoustic_energy'] = X_featured['acousticness'] * X_featured['energy']\n",
    "    X_featured['dance_valence'] = X_featured['danceability'] * X_featured['audio_valence']\n",
    "    X_featured['loudness_energy'] = X_featured['loudness'] * X_featured['energy']\n",
    "    X_featured['speechiness_energy'] = X_featured['speechiness'] * X_featured['energy']\n",
    "    \n",
    "    # 2. Ratio features\n",
    "    X_featured['energy_loudness_ratio'] = X_featured['energy'] / (abs(X_featured['loudness']) + 1e-5)\n",
    "    X_featured['danceability_energy_ratio'] = X_featured['danceability'] / (X_featured['energy'] + 1e-5)\n",
    "    X_featured['acousticness_energy_ratio'] = X_featured['acousticness'] / (X_featured['energy'] + 1e-5)\n",
    "    X_featured['valence_energy_ratio'] = X_featured['audio_valence'] / (X_featured['energy'] + 1e-5)\n",
    "    \n",
    "    # 3. Polynomial features\n",
    "    X_featured['energy_squared'] = X_featured['energy'] ** 2\n",
    "    X_featured['danceability_squared'] = X_featured['danceability'] ** 2\n",
    "    X_featured['loudness_squared'] = X_featured['loudness'] ** 2\n",
    "    X_featured['tempo_squared'] = X_featured['tempo'] ** 2\n",
    "    \n",
    "    # 4. Duration features\n",
    "    X_featured['duration_minutes'] = X_featured['song_duration_ms'] / 60000\n",
    "    X_featured['duration_seconds'] = X_featured['song_duration_ms'] / 1000\n",
    "    X_featured['log_duration'] = np.log1p(X_featured['song_duration_ms'])\n",
    "    \n",
    "    # 5. Loudness features\n",
    "    X_featured['loudness_abs'] = abs(X_featured['loudness'])\n",
    "    X_featured['loudness_normalized'] = (X_featured['loudness'] - X_featured['loudness'].min()) / (X_featured['loudness'].max() - X_featured['loudness'].min() + 1e-5)\n",
    "    \n",
    "    # 6. Tempo features (keep as numerical)\n",
    "    X_featured['tempo_zscore'] = (X_featured['tempo'] - X_featured['tempo'].mean()) / (X_featured['tempo'].std() + 1e-5)\n",
    "    \n",
    "    # 7. Audio feature combinations\n",
    "    X_featured['audio_intensity'] = X_featured['energy'] + X_featured['loudness'] / 10\n",
    "    X_featured['melodic_complexity'] = X_featured['instrumentalness'] + X_featured['speechiness']\n",
    "    X_featured['emotional_tone'] = X_featured['audio_valence'] * (1 - X_featured['acousticness'])\n",
    "    \n",
    "    # 8. Statistical features\n",
    "    X_featured['feature_sum'] = X_featured[['energy', 'danceability', 'loudness']].sum(axis=1)\n",
    "    X_featured['feature_mean'] = X_featured[['energy', 'danceability', 'loudness', 'audio_valence']].mean(axis=1)\n",
    "    X_featured['feature_std'] = X_featured[['energy', 'danceability', 'loudness']].std(axis=1)\n",
    "    \n",
    "    # 9. Binning features (convert to numerical for XGBoost compatibility)\n",
    "    X_featured['energy_bin'] = pd.qcut(X_featured['energy'], q=5, labels=False, duplicates='drop').astype(int)\n",
    "    X_featured['danceability_bin'] = pd.qcut(X_featured['danceability'], q=5, labels=False, duplicates='drop').astype(int)\n",
    "    X_featured['valence_bin'] = pd.qcut(X_featured['audio_valence'], q=5, labels=False, duplicates='drop').astype(int)\n",
    "    \n",
    "    # 10. Interaction with categorical features (convert to numerical)\n",
    "    X_featured['energy_key'] = X_featured['energy'] * X_featured['key']\n",
    "    X_featured['danceability_mode'] = X_featured['danceability'] * X_featured['audio_mode']\n",
    "    \n",
    "    # Convert original categorical features to numerical\n",
    "    X_featured['key'] = X_featured['key'].astype(int)\n",
    "    X_featured['audio_mode'] = X_featured['audio_mode'].astype(int)\n",
    "    X_featured['time_signature'] = X_featured['time_signature'].astype(int)\n",
    "    \n",
    "    return X_featured\n",
    "\n",
    "def preprocess_data(df, is_train=True, imputer_dict=None, scaler_dict=None, feature_selector=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data with proper handling for train/test sets\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Separate features and target if it's training data\n",
    "    if is_train:\n",
    "        X = df_processed.drop('song_popularity', axis=1)\n",
    "    else:\n",
    "        X = df_processed\n",
    "    \n",
    "    # Handle missing values\n",
    "    numerical_features = ['song_duration_ms', 'acousticness', 'danceability', 'energy', \n",
    "                         'instrumentalness', 'liveness', 'loudness', 'speechiness', \n",
    "                         'tempo', 'audio_valence']\n",
    "    \n",
    "    categorical_features = ['key', 'time_signature', 'audio_mode']\n",
    "    \n",
    "    # Initialize imputers if not provided\n",
    "    if imputer_dict is None:\n",
    "        imputer_dict = {\n",
    "            'num_imputer': KNNImputer(n_neighbors=7),\n",
    "            'cat_imputer': SimpleImputer(strategy='most_frequent')\n",
    "        }\n",
    "    \n",
    "    # Apply imputation\n",
    "    X[numerical_features] = imputer_dict['num_imputer'].fit_transform(X[numerical_features]) if is_train else imputer_dict['num_imputer'].transform(X[numerical_features])\n",
    "    X[categorical_features] = imputer_dict['cat_imputer'].fit_transform(X[categorical_features]) if is_train else imputer_dict['cat_imputer'].transform(X[categorical_features])\n",
    "    \n",
    "    # Advanced feature engineering\n",
    "    X = advanced_feature_engineering(X)\n",
    "    \n",
    "    # Identify numerical features after engineering\n",
    "    all_numerical = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove ID from numerical features\n",
    "    if 'id' in all_numerical:\n",
    "        all_numerical.remove('id')\n",
    "    \n",
    "    # Scale numerical features with RobustScaler\n",
    "    if scaler_dict is None:\n",
    "        scaler_dict = {\n",
    "            'scaler': RobustScaler()\n",
    "        }\n",
    "    \n",
    "    X_scaled = scaler_dict['scaler'].fit_transform(X[all_numerical]) if is_train else scaler_dict['scaler'].transform(X[all_numerical])\n",
    "    \n",
    "    # Create final DataFrame with ALL features first\n",
    "    X_final = pd.DataFrame(X_scaled, columns=all_numerical, index=X.index)\n",
    "    \n",
    "    # Store all feature names before selection for reference\n",
    "    all_feature_names = X_final.columns.tolist()\n",
    "    \n",
    "    # Feature selection (only on training)\n",
    "    if is_train and feature_selector is None:\n",
    "        selector = SelectFromModel(\n",
    "            XGBClassifier(random_state=42, n_estimators=100),\n",
    "            threshold='median'\n",
    "        )\n",
    "        selector.fit(X_final, y_train_binary)\n",
    "        feature_selector = selector\n",
    "        # Store the selected feature names\n",
    "        selected_features = X_final.columns[selector.get_support()].tolist()\n",
    "        feature_selector.selected_features_ = selected_features\n",
    "        print(f\"Selected {len(selected_features)} features out of {len(all_feature_names)}\")\n",
    "    \n",
    "    # Apply feature selection to both train and test\n",
    "    if feature_selector is not None:\n",
    "        if hasattr(feature_selector, 'selected_features_'):\n",
    "            # Use the stored feature names\n",
    "            X_final = X_final[feature_selector.selected_features_]\n",
    "        else:\n",
    "            # Fallback to selector method\n",
    "            X_final = X_final.loc[:, feature_selector.get_support()]\n",
    "    \n",
    "    return X_final, imputer_dict, scaler_dict, feature_selector\n",
    "\n",
    "# Preprocess training data\n",
    "X_train, imputer_dict, scaler_dict, feature_selector = preprocess_data(train_df, is_train=True)\n",
    "\n",
    "# Preprocess test data - make sure to use the same feature_selector\n",
    "X_test, _, _, _ = preprocess_data(test_df, is_train=False, \n",
    "                                 imputer_dict=imputer_dict, \n",
    "                                 scaler_dict=scaler_dict,\n",
    "                                 feature_selector=feature_selector)\n",
    "\n",
    "print(\"Processed train features shape:\", X_train.shape)\n",
    "print(\"Processed test features shape:\", X_test.shape)\n",
    "print(\"Train features:\", X_train.columns.tolist())\n",
    "print(\"Test features:\", X_test.columns.tolist())\n",
    "\n",
    "# Verify feature alignment\n",
    "if not X_train.columns.equals(X_test.columns):\n",
    "    print(\"WARNING: Feature mismatch detected!\")\n",
    "    print(\"Missing in test:\", set(X_train.columns) - set(X_test.columns))\n",
    "    print(\"Extra in test:\", set(X_test.columns) - set(X_train.columns))\n",
    "    \n",
    "    # Align features manually\n",
    "    common_features = list(set(X_train.columns) & set(X_test.columns))\n",
    "    X_train = X_train[common_features]\n",
    "    X_test = X_test[common_features]\n",
    "    print(f\"Using {len(common_features)} common features\")\n",
    "\n",
    "# ... (rest of the code remains the same until model training)\n",
    "\n",
    "# Define a much more efficient ensemble class\n",
    "class FastWeightedEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models=None, weights=None):\n",
    "        self.models = models if models is not None else []\n",
    "        self.weights = weights if weights is not None else []\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'models': self.models,\n",
    "            'weights': self.weights\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "    \n",
    "    def _calculate_fast_weights(self, X, y):\n",
    "        \"\"\"Fast weight calculation using quick CV\"\"\"\n",
    "        scores = []\n",
    "        for name, model in self.models:\n",
    "            # Quick 2-fold CV for speed\n",
    "            try:\n",
    "                cv_score = cross_val_score(model, X, y, cv=2, scoring='roc_auc', n_jobs=-1).mean()\n",
    "            except:\n",
    "                # If CV fails, use simple score\n",
    "                model.fit(X, y)\n",
    "                y_pred = model.predict_proba(X)[:, 1]\n",
    "                cv_score = roc_auc_score(y, y_pred)\n",
    "            scores.append(cv_score)\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        # Add small epsilon to avoid zero weights\n",
    "        weights = (scores + 0.01) / (scores.sum() + 0.01 * len(scores))\n",
    "        return weights.tolist()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if not self.weights:\n",
    "            self.weights = self._calculate_fast_weights(X, y)\n",
    "        \n",
    "        # Fit models sequentially (more reliable than parallel for this case)\n",
    "        fitted_models = []\n",
    "        for name, model in self.models:\n",
    "            print(f\"Fitting {name}...\")\n",
    "            fitted_model = clone(model)\n",
    "            fitted_model.fit(X, y)\n",
    "            fitted_models.append((name, fitted_model))\n",
    "        \n",
    "        self.models = fitted_models\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Predict sequentially\n",
    "        probas = []\n",
    "        for name, model in self.models:\n",
    "            proba = model.predict_proba(X)[:, 1]\n",
    "            probas.append(proba)\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_sum = np.zeros_like(probas[0])\n",
    "        for proba, weight in zip(probas, self.weights):\n",
    "            weighted_sum += proba * weight\n",
    "        \n",
    "        return np.column_stack([1 - weighted_sum, weighted_sum])\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "# Train individual models with optimized settings\n",
    "print(\"Training individual models with optimized settings...\")\n",
    "\n",
    "# XGBoost with faster parameters\n",
    "xgb_model_1 = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=2.0,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_model_2 = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.06,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=2.5,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# CatBoost with faster parameters\n",
    "catboost_model_1 = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    auto_class_weights='Balanced',\n",
    "    thread_count=-1\n",
    ")\n",
    "\n",
    "# Train models\n",
    "individual_models = {\n",
    "    'XGBoost_1': xgb_model_1,\n",
    "    'XGBoost_2': xgb_model_2,\n",
    "    'CatBoost_1': catboost_model_1\n",
    "}\n",
    "\n",
    "for name, model in individual_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train_binary)\n",
    "    # Quick score calculation\n",
    "    y_pred = model.predict_proba(X_train)[:, 1]\n",
    "    train_score = roc_auc_score(y_train_binary, y_pred)\n",
    "    print(f\"{name}: Train AUC = {train_score:.4f}\")\n",
    "\n",
    "# Create ensemble with pre-trained models\n",
    "base_models = [\n",
    "    ('xgb1', xgb_model_1),\n",
    "    ('xgb2', xgb_model_2),\n",
    "    ('cat1', catboost_model_1)\n",
    "]\n",
    "\n",
    "# Create and train ensemble quickly\n",
    "print(\"\\nCreating fast ensemble...\")\n",
    "ensemble = FastWeightedEnsemble(models=base_models)\n",
    "ensemble.fit(X_train, y_train_binary)\n",
    "\n",
    "# Quick evaluation\n",
    "train_pred_proba = ensemble.predict_proba(X_train)[:, 1]\n",
    "train_auc = roc_auc_score(y_train_binary, train_pred_proba)\n",
    "optimal_threshold = find_optimal_threshold(y_train_binary, train_pred_proba)\n",
    "\n",
    "print(f\"Ensemble Train AUC: {train_auc:.4f}\")\n",
    "print(f\"Optimal threshold: {optimal_threshold:.6f}\")\n",
    "\n",
    "# Use a simple holdout validation for quick ensemble evaluation\n",
    "X_train_fast, X_val_fast, y_train_fast, y_val_fast = train_test_split(\n",
    "    X_train, y_train_binary, test_size=0.2, random_state=42, stratify=y_train_binary\n",
    ")\n",
    "\n",
    "# Quick ensemble validation\n",
    "val_ensemble = FastWeightedEnsemble(models=base_models)\n",
    "val_ensemble.fit(X_train_fast, y_train_fast)\n",
    "val_pred_proba = val_ensemble.predict_proba(X_val_fast)[:, 1]\n",
    "val_auc = roc_auc_score(y_val_fast, val_pred_proba)\n",
    "\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Final ensemble - use the one trained on full data\n",
    "final_ensemble = ensemble\n",
    "final_pred_proba = final_ensemble.predict_proba(X_train)[:, 1]\n",
    "final_auc = roc_auc_score(y_train_binary, final_pred_proba)\n",
    "final_threshold = find_optimal_threshold(y_train_binary, final_pred_proba)\n",
    "\n",
    "print(f\"\\nFinal Ensemble Performance:\")\n",
    "print(f\"Train AUC: {final_auc:.4f}\")\n",
    "print(f\"Optimal threshold: {final_threshold:.6f}\")\n",
    "\n",
    "# Save the final ensemble\n",
    "joblib.dump(final_ensemble, 'fast_ensemble_model.pkl')\n",
    "joblib.dump(imputer_dict, 'imputer_dict.pkl')\n",
    "joblib.dump(scaler_dict, 'scaler_dict.pkl')\n",
    "joblib.dump(feature_selector, 'feature_selector.pkl')\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "test_pred_proba = final_ensemble.predict_proba(X_test)[:, 1]\n",
    "test_predictions = (test_pred_proba >= final_threshold).astype(int)\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'song_popularity': test_predictions\n",
    "})\n",
    "\n",
    "# Save Probabilities as well\n",
    "submission_df_proba = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'song_popularity': test_pred_proba\n",
    "})\n",
    "submission_df_proba.to_csv(\"submission_8_probabilities.csv\", index=False)\n",
    "\n",
    "# Save predictions\n",
    "file_output_name = \"submission_8.csv\"\n",
    "submission_df.to_csv(file_output_name, index=False)\n",
    "print(f\"Submission file '{file_output_name}' created!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
