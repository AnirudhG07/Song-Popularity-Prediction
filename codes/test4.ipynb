{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10928670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load train and test data\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nTrain data info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Check the target variable distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['song_popularity'], bins=30, kde=True)\n",
    "plt.title('Distribution of Song Popularity')\n",
    "plt.show()\n",
    "\n",
    "def preprocess_data(df, is_train=True, imputer_dict=None, scaler_dict=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data with proper handling for train/test sets\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Separate features and target if it's training data\n",
    "    if is_train:\n",
    "        X = df_processed.drop('song_popularity', axis=1)\n",
    "        y = df_processed['song_popularity']\n",
    "    else:\n",
    "        X = df_processed\n",
    "        y = None\n",
    "    \n",
    "    # Handle missing values\n",
    "    numerical_features = ['song_duration_ms', 'acousticness', 'danceability', 'energy', \n",
    "                         'instrumentalness', 'liveness', 'loudness', 'speechiness', \n",
    "                         'tempo', 'audio_valence']\n",
    "    \n",
    "    categorical_features = ['key', 'time_signature', 'audio_mode']\n",
    "    \n",
    "    # Initialize imputers if not provided\n",
    "    if imputer_dict is None:\n",
    "        imputer_dict = {\n",
    "            'num_imputer': KNNImputer(n_neighbors=5),\n",
    "            'cat_imputer': SimpleImputer(strategy='most_frequent')\n",
    "        }\n",
    "    \n",
    "    # Apply imputation\n",
    "    X[numerical_features] = imputer_dict['num_imputer'].fit_transform(X[numerical_features]) if is_train else imputer_dict['num_imputer'].transform(X[numerical_features])\n",
    "    X[categorical_features] = imputer_dict['cat_imputer'].fit_transform(X[categorical_features]) if is_train else imputer_dict['cat_imputer'].transform(X[categorical_features])\n",
    "    \n",
    "    # Feature engineering\n",
    "    X = create_features(X)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    numerical_features_extended = [f for f in X.columns if f not in categorical_features + ['id']]\n",
    "    \n",
    "    if scaler_dict is None:\n",
    "        scaler_dict = {\n",
    "            'scaler': StandardScaler()\n",
    "        }\n",
    "    \n",
    "    X_scaled = scaler_dict['scaler'].fit_transform(X[numerical_features_extended]) if is_train else scaler_dict['scaler'].transform(X[numerical_features_extended])\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    X_final = pd.DataFrame(X_scaled, columns=numerical_features_extended, index=X.index)\n",
    "    X_final[categorical_features] = X[categorical_features].values\n",
    "    \n",
    "    return X_final, y, imputer_dict, scaler_dict\n",
    "\n",
    "def create_features(X):\n",
    "    \"\"\"Create additional features\"\"\"\n",
    "    X_featured = X.copy()\n",
    "    \n",
    "    # Interaction features\n",
    "    X_featured['energy_danceability'] = X_featured['energy'] * X_featured['danceability']\n",
    "    X_featured['valence_energy'] = X_featured['audio_valence'] * X_featured['energy']\n",
    "    X_featured['acoustic_energy'] = X_featured['acousticness'] * X_featured['energy']\n",
    "    X_featured['dance_valence'] = X_featured['danceability'] * X_featured['audio_valence']\n",
    "    \n",
    "    # Duration features\n",
    "    X_featured['duration_minutes'] = X_featured['song_duration_ms'] / 60000\n",
    "    X_featured['duration_seconds'] = X_featured['song_duration_ms'] / 1000\n",
    "    \n",
    "    # Loudness features\n",
    "    X_featured['loudness_abs'] = abs(X_featured['loudness'])\n",
    "    \n",
    "    # Tempo features\n",
    "    X_featured['tempo_category'] = pd.cut(X_featured['tempo'], \n",
    "                                        bins=[0, 60, 100, 140, 200, 300],\n",
    "                                        labels=[0, 1, 2, 3, 4])\n",
    "    X_featured['tempo_category'] = X_featured['tempo_category'].astype('category')\n",
    "    \n",
    "    # Energy ratio features\n",
    "    X_featured['energy_loudness_ratio'] = X_featured['energy'] / (abs(X_featured['loudness']) + 1)\n",
    "    \n",
    "    return X_featured\n",
    "\n",
    "# Preprocess training data\n",
    "X_train, y_train, imputer_dict, scaler_dict = preprocess_data(train_df, is_train=True)\n",
    "\n",
    "# Preprocess test data using the same imputers and scaler from training\n",
    "X_test, y_test, _, _ = preprocess_data(test_df, is_train=False, \n",
    "                                      imputer_dict=imputer_dict, \n",
    "                                      scaler_dict=scaler_dict)\n",
    "\n",
    "print(\"Processed train features shape:\", X_train.shape)\n",
    "print(\"Processed test features shape:\", X_test.shape)\n",
    "\n",
    "# Define multiple models with different algorithms\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    'CatBoost': CatBoostRegressor(n_estimators=200, random_state=42, verbose=0),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.01, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training data for validation\n",
    "    y_pred = model.predict(X_train)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred)\n",
    "    train_mse = mean_squared_error(y_train, y_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'train_mse': train_mse,\n",
    "        'train_mae': train_mae,\n",
    "        'cv_mean_r2': cv_scores.mean(),\n",
    "        'cv_std_r2': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}: Train R² = {train_r2:.4f}, CV R² = {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df.sort_values('cv_mean_r2', ascending=False))\n",
    "\n",
    "# Create base models for ensemble\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('xgb', XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('lgbm', LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Voting Ensemble\n",
    "voting_ensemble = VotingRegressor(estimators=base_models, n_jobs=-1)\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Stacking Ensemble\n",
    "stacking_ensemble = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=GradientBoostingRegressor(n_estimators=50, random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ensembles\n",
    "ensemble_results = {}\n",
    "for name, ensemble in [('Voting', voting_ensemble), ('Stacking', stacking_ensemble)]:\n",
    "    y_pred = ensemble.predict(X_train)\n",
    "    cv_scores = cross_val_score(ensemble, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "    \n",
    "    ensemble_results[name] = {\n",
    "        'train_r2': r2_score(y_train, y_pred),\n",
    "        'cv_mean_r2': cv_scores.mean(),\n",
    "        'cv_std_r2': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name} Ensemble: R² = {r2_score(y_train, y_pred):.4f}, CV R² = {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Add to results\n",
    "ensemble_results_df = pd.DataFrame(ensemble_results).T\n",
    "results_df = pd.concat([results_df, ensemble_results_df])\n",
    "\n",
    "# Let's tune the best performing model (usually XGBoost or LightGBM)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning\n",
    "xgb_tuned = RandomizedSearchCV(\n",
    "    XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_tuned.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost parameters:\", xgb_tuned.best_params_)\n",
    "print(\"Best XGBoost score:\", xgb_tuned.best_score_)\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = XGBRegressor(\n",
    "    **xgb_tuned.best_params_,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'song_popularity': test_predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "file_output_name = \"submission_1.csv\"\n",
    "submission_df.to_csv(file_output_name, index=False)\n",
    "print(f\"Submission file '{file_output_name}' created!\")\n",
    "\n",
    "# Evaluate final model (if test has actual values)\n",
    "if 'song_popularity' in test_df.columns:\n",
    "    test_actual = test_df['song_popularity']\n",
    "    test_r2 = r2_score(test_actual, test_predictions)\n",
    "    test_mse = mean_squared_error(test_actual, test_predictions)\n",
    "    print(f\"Final Test Performance: R² = {test_r2:.4f}, MSE = {test_mse:.4f}\")\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Create a summary of key insights\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1. Top Predictive Features:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Model Performance Summary:\")\n",
    "best_models = results_df.sort_values('cv_mean_r2', ascending=False).head(3)\n",
    "for model_name in best_models.index:\n",
    "    score = best_models.loc[model_name, 'cv_mean_r2']\n",
    "    print(f\"   {model_name}: R² = {score:.4f}\")\n",
    "\n",
    "print(\"\\n3. Key Factors for Song Popularity:\")\n",
    "print(\"   - Danceability and energy are typically strong predictors\")\n",
    "print(\"   - Loudness and tempo often contribute significantly\")\n",
    "print(\"   - Valence (musical positiveness) can influence popularity\")\n",
    "print(\"   - Duration and acousticness may have complex relationships\")\n",
    "\n",
    "print(\"\\n4. Recommendations for Improvement:\")\n",
    "print(\"   - Try additional feature engineering\")\n",
    "print(\"   - Experiment with different scaling methods\")\n",
    "print(\"   - Consider neural networks for complex patterns\")\n",
    "print(\"   - Use feature selection to reduce dimensionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ./submission_1.py, and if song_popularity  > 0.37, put it 1, else 0\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./submission_1.csv')\n",
    "df['song_popularity'] = df['song_popularity'].apply(lambda x: 1 if x > 0.365052 else 0)\n",
    "df.to_csv('./submission_4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
